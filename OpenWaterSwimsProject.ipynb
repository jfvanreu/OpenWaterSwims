{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Open Water Swims\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "**Open Water Swims** is an application for open water swimmers. Swimmers can \"check-in or check-out\" to different swims. In the context of this project, we only track open water swims in the United States but it could be extended worldwide in the future.\n",
    "\n",
    "The purpose of the project is to *extract* data from a swimslog which records all events where swimmers have checked in or checked out for swims. The project will also connect to publicly available data about buoys and water conditions (both provided by NOAA) to track the conditions associated to those swims, when available. In this project, we also have access to information about the available swims for which swimmers checked in/out.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* **Step 1:** Scope the Project and Gather Data\n",
    "* **Step 2:** Explore and Assess the Data\n",
    "* **Step 3:** Define the Data Model\n",
    "* **Step 4:** Run ETL to Model the Data\n",
    "* **Step 5:** Run ETL on AWS EMR\n",
    "* **Step 6:** Other considerations\n",
    "* **Step 7:** Final Thoughts\n",
    "\n",
    "The project will rely on the following Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Packages to parse data files\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import urllib\n",
    "from datetime import datetime\n",
    "\n",
    "# spark packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, ShortType as Short, \\\n",
    "    StringType as Str, FloatType as Float, DoubleType as Dbl, LongType as Long, TimestampType as TStamp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope\n",
    "The purpose of this project is to create a database model for an open water swims application. The project will extract  data from a swimming events log. It will also leverage publicly available data provided by NOAA. The purpose of this project is to create a **STAR** database schema, so it can be accessed by the application quickly and efficiently.\n",
    "\n",
    "#### Technologies\n",
    "As part of this project, we use the following technologies:\n",
    "\n",
    "- **Pandas**: Pandas is a great tool to explore small data sets; it allows us to explore the structure and volume of the data.\n",
    "- **Apache Spark**: Apache Spark is used to process *big data* sets. We first use it in *local* mode to validate our ETL process; then we use Apache Spark on an EMR cluster to process big data sets.\n",
    "- **AWS EMR**: AWS clusters that we use to run Apache Spark on the big data set.\n",
    "- **AWS S3**: used to store big data sets and fact/dimension tables created by this project.\n",
    "- **AWS Redshift**: data warehouse where we load our dimension tables so they can be explored using visualization tools such as Tableau.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "We used the following public data sets to support our project:\n",
    "\n",
    "- ***NOOA Buoy Data*** -- which is a .xml file which includes all the NOAA stations available, their location, owner, etc...This is a fairly small data set.\n",
    "- ***NOAA Log file*** -- which provides ocean related data (water temp, air temp, wave height, wind dir, wind, ...) since 1970. In the context of this project, we'll only work with data since 2000. \n",
    "\n",
    "**NOAA** provided a great document to describe how to access their data. It's available [here](https://www.ndbc.noaa.gov/docs/ndbc_web_data_guide.pdf)\n",
    "\n",
    "- ***Swims*** -- is a \"fake\" generated database which includes information on all the open water swims in United States. Each swim is scheduled to happen at a certain time and location. It is associated to a specific NOAA buoy, which provides us information on the water conditions. We have access to a history of swims since 2000. The data is organized by months and years. The dataset includes 5000 swims per month since 2000. The database includes **1.2 Million** swims).\n",
    "\n",
    "- ***SwimsLog*** -- is also another artificially generated file which I created to simulate the events with the swims. This tracks when swimmers check in or check out for a swim. Each SwimsLog record provides the following information: an eventId, the timestamp of this event, swimmer first and last names, the status (whether the swimmer checked in or out), the location (latitude/longitude) of the swimmer when he/she checked in/out and the swimID for which the swimmer checked in/out. There are **2.4 Million** records for Swimslog.\n",
    "\n",
    "The Swims and SwimsLog datasets were created by using http://www.generatedata.com/ pseudo-data provider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "In this section, we'll explore the data sets to identify their structure, fields and potential quality issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Active Buoys Data\n",
    "**NOAA** provides two key data sets : \n",
    "1. an xml file with the active buoys on their web-site and \n",
    "2. historical data associated to each buoy.\n",
    "\n",
    "Let's first identify the active buoys. NOAA provides the information in a fairly small XML file. Since this is a small data set, we'll work with Pandas data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1435 active stations\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>currents</th>\n",
       "      <th>dart</th>\n",
       "      <th>elev</th>\n",
       "      <th>id</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>met</th>\n",
       "      <th>name</th>\n",
       "      <th>owner</th>\n",
       "      <th>pgm</th>\n",
       "      <th>seq</th>\n",
       "      <th>type</th>\n",
       "      <th>waterquality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00922</td>\n",
       "      <td>30</td>\n",
       "      <td>-90</td>\n",
       "      <td>n</td>\n",
       "      <td>OTN201 - 4800922</td>\n",
       "      <td>Dalhousie University</td>\n",
       "      <td>IOOS Partners</td>\n",
       "      <td>NaN</td>\n",
       "      <td>other</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00923</td>\n",
       "      <td>30</td>\n",
       "      <td>-90</td>\n",
       "      <td>n</td>\n",
       "      <td>OTN200 - 4800923</td>\n",
       "      <td>Dalhousie University</td>\n",
       "      <td>IOOS Partners</td>\n",
       "      <td>NaN</td>\n",
       "      <td>other</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01500</td>\n",
       "      <td>30</td>\n",
       "      <td>-90</td>\n",
       "      <td>n</td>\n",
       "      <td>SP031 - 3801500</td>\n",
       "      <td>SCRIPPS</td>\n",
       "      <td>IOOS Partners</td>\n",
       "      <td>NaN</td>\n",
       "      <td>other</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01502</td>\n",
       "      <td>30</td>\n",
       "      <td>-90</td>\n",
       "      <td>n</td>\n",
       "      <td>Penobscot - 4801502</td>\n",
       "      <td>University of Maine</td>\n",
       "      <td>IOOS Partners</td>\n",
       "      <td>NaN</td>\n",
       "      <td>other</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01503</td>\n",
       "      <td>30</td>\n",
       "      <td>-90</td>\n",
       "      <td>n</td>\n",
       "      <td>Saul - 4801503</td>\n",
       "      <td>Woods Hole Oceanographic Institution</td>\n",
       "      <td>IOOS Partners</td>\n",
       "      <td>NaN</td>\n",
       "      <td>other</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  currents dart elev     id lat  lon met                 name  \\\n",
       "0        n    n  NaN  00922  30  -90   n     OTN201 - 4800922   \n",
       "1        n    n  NaN  00923  30  -90   n     OTN200 - 4800923   \n",
       "2        n    n  NaN  01500  30  -90   n      SP031 - 3801500   \n",
       "3        n    n  NaN  01502  30  -90   n  Penobscot - 4801502   \n",
       "4        n    n  NaN  01503  30  -90   n       Saul - 4801503   \n",
       "\n",
       "                                  owner            pgm  seq   type  \\\n",
       "0                  Dalhousie University  IOOS Partners  NaN  other   \n",
       "1                  Dalhousie University  IOOS Partners  NaN  other   \n",
       "2                               SCRIPPS  IOOS Partners  NaN  other   \n",
       "3                   University of Maine  IOOS Partners  NaN  other   \n",
       "4  Woods Hole Oceanographic Institution  IOOS Partners  NaN  other   \n",
       "\n",
       "  waterquality  \n",
       "0            n  \n",
       "1            n  \n",
       "2            n  \n",
       "3            n  \n",
       "4            n  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buoys_url='https://www.ndbc.noaa.gov/activestations.xml'\n",
    "\n",
    "response = urllib.request.urlopen(buoys_url).read()\n",
    "root = ET.fromstring(response)\n",
    "\n",
    "stations=root.getchildren()\n",
    "print('There are {} active stations'.format(len(stations)))\n",
    "d = []\n",
    "for station in stations:\n",
    "    d.append(station.attrib)\n",
    "    \n",
    "df_buoys = pd.DataFrame(d)\n",
    "df_buoys.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Buoy historical data\n",
    "**NOAA** also provides historical data for its buoys. Data is available at the following URL: https://www.ndbc.noaa.gov/data/historical/stdmet/.\n",
    "The filename starts with the buoy ID followed by the letter 'h' and the year for which the data is available. In the example below, we're accessing the 2008 data for the buoy 41004. If we look at the active buoy data (from previous step), we would know who owns this buoy and where it is located.\n",
    "Note that the buoy ID was only included in the filename (and not in the data), so we decided to extract that information and added a column buoyID to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YYYY</th>\n",
       "      <th>MM</th>\n",
       "      <th>DD</th>\n",
       "      <th>hh</th>\n",
       "      <th>WD</th>\n",
       "      <th>WSPD</th>\n",
       "      <th>GST</th>\n",
       "      <th>WVHT</th>\n",
       "      <th>DPD</th>\n",
       "      <th>APD</th>\n",
       "      <th>MWD</th>\n",
       "      <th>BAR</th>\n",
       "      <th>ATMP</th>\n",
       "      <th>WTMP</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>VIS</th>\n",
       "      <th>TIDE</th>\n",
       "      <th>buoyID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>999</td>\n",
       "      <td>1020.3</td>\n",
       "      <td>20.4</td>\n",
       "      <td>20.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>123</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>999</td>\n",
       "      <td>1020.3</td>\n",
       "      <td>20.4</td>\n",
       "      <td>20.7</td>\n",
       "      <td>15.5</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>133</td>\n",
       "      <td>3.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>999</td>\n",
       "      <td>1020.5</td>\n",
       "      <td>20.4</td>\n",
       "      <td>20.7</td>\n",
       "      <td>16.3</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>134</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>999</td>\n",
       "      <td>1020.5</td>\n",
       "      <td>20.3</td>\n",
       "      <td>20.6</td>\n",
       "      <td>16.6</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>145</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.9</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>999</td>\n",
       "      <td>1020.3</td>\n",
       "      <td>20.4</td>\n",
       "      <td>20.6</td>\n",
       "      <td>17.4</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YYYY  MM  DD  hh   WD  WSPD  GST  WVHT   DPD   APD  MWD     BAR  ATMP  \\\n",
       "0  2000   1   1   1  141   1.8  2.2  99.0  99.0  99.0  999  1020.3  20.4   \n",
       "1  2000   1   1   2  123   3.5  4.0  99.0  99.0  99.0  999  1020.3  20.4   \n",
       "2  2000   1   1   3  133   3.8  4.6  99.0  99.0  99.0  999  1020.5  20.4   \n",
       "3  2000   1   1   4  134   4.2  4.9  99.0  99.0  99.0  999  1020.5  20.3   \n",
       "4  2000   1   1   5  145   4.3  4.9  99.0  99.0  99.0  999  1020.3  20.4   \n",
       "\n",
       "   WTMP  DEWP   VIS  TIDE buoyID  \n",
       "0  20.7  14.6  99.0   NaN  42036  \n",
       "1  20.7  15.5  99.0   NaN  42036  \n",
       "2  20.7  16.3  99.0   NaN  42036  \n",
       "3  20.6  16.6  99.0   NaN  42036  \n",
       "4  20.6  17.4  99.0   NaN  42036  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read a sample buoy data log file to check the format/content\n",
    "url = 'https://www.ndbc.noaa.gov/data/historical/stdmet/42036h2000.txt.gz'\n",
    "ext = '.txt.gz'\n",
    "\n",
    "df = pd.read_csv(url, sep='\\s+')\n",
    "\n",
    "# extract the buoyID from the file path then add it as a column to the Dataframe.\n",
    "buoyID=url.rsplit('h', 1)[0].split('/')[-1]\n",
    "df['buoyID'] = buoyID\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "YYYY      8783\n",
       "MM        8783\n",
       "DD        8783\n",
       "hh        8783\n",
       "WD        8783\n",
       "WSPD      8783\n",
       "GST       8783\n",
       "WVHT      8783\n",
       "DPD       8783\n",
       "APD       8783\n",
       "MWD       8783\n",
       "BAR       8783\n",
       "ATMP      8783\n",
       "WTMP      8783\n",
       "DEWP      8783\n",
       "VIS       8783\n",
       "TIDE      3672\n",
       "buoyID    8783\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Above, we can see the data provided for each buoy. When information is unavailable, NOAA provides 999 or 99 as an entry.\n",
    "This varies by buoy and time. For example, for a period of time, there is no collection of the water temperature.\n",
    "Note that the format varied over time. We also came across some table formats where the header and first rows were commented out (using the # character). In those cases, the units associated to each column were provided. We also discovered that some buoy tables were empty. See below the \"commented out\" format for a buoy with no data. Our ETL pipeline will need to address those situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read a sample buoy data log file to check the format/content\n",
    "url = 'https://www.ndbc.noaa.gov/data/historical/stdmet/43WSLh2019.txt.gz'\n",
    "ext = '.txt.gz'\n",
    "\n",
    "df2 = pd.read_csv(url, sep='\\s+')\n",
    "\n",
    "# extract the buoyID from the file path then add it as a column to the Dataframe.\n",
    "buoyID=url.rsplit('h', 1)[0].split('/')[-1]\n",
    "df2['buoyID'] = buoyID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#YY</th>\n",
       "      <th>MM</th>\n",
       "      <th>DD</th>\n",
       "      <th>hh</th>\n",
       "      <th>mm</th>\n",
       "      <th>WDIR</th>\n",
       "      <th>WSPD</th>\n",
       "      <th>GST</th>\n",
       "      <th>WVHT</th>\n",
       "      <th>DPD</th>\n",
       "      <th>APD</th>\n",
       "      <th>MWD</th>\n",
       "      <th>PRES</th>\n",
       "      <th>ATMP</th>\n",
       "      <th>WTMP</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>VIS</th>\n",
       "      <th>TIDE</th>\n",
       "      <th>buoyID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#yr</td>\n",
       "      <td>mo</td>\n",
       "      <td>dy</td>\n",
       "      <td>hr</td>\n",
       "      <td>mn</td>\n",
       "      <td>degT</td>\n",
       "      <td>m/s</td>\n",
       "      <td>m/s</td>\n",
       "      <td>m</td>\n",
       "      <td>sec</td>\n",
       "      <td>sec</td>\n",
       "      <td>degT</td>\n",
       "      <td>hPa</td>\n",
       "      <td>degC</td>\n",
       "      <td>degC</td>\n",
       "      <td>degC</td>\n",
       "      <td>mi</td>\n",
       "      <td>ft</td>\n",
       "      <td>43WSL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #YY  MM  DD  hh  mm  WDIR WSPD  GST WVHT  DPD  APD   MWD PRES  ATMP  WTMP  \\\n",
       "0  #yr  mo  dy  hr  mn  degT  m/s  m/s    m  sec  sec  degT  hPa  degC  degC   \n",
       "\n",
       "   DEWP VIS TIDE buoyID  \n",
       "0  degC  mi   ft  43WSL  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "NOAA provides all historical information for all its buoys in a specific folder. Let's check how many files we'll need to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11167 files since year 2000.\n"
     ]
    }
   ],
   "source": [
    "# Read the buoy data log here\n",
    "url = 'https://www.ndbc.noaa.gov/data/historical/stdmet/'\n",
    "ext = '.txt.gz'\n",
    "\n",
    "# get the list of data files\n",
    "page = requests.get(url).text\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "fileList = [url + '/' + node.get('href') for node in soup.find_all('a') if node.get('href').endswith(ext)]\n",
    "\n",
    "#create year filter\n",
    "yearFilter=range(2000, 2025)\n",
    "yearFilter=['h' + str(s) for s in yearFilter]\n",
    "filteredFileList = [n for n in fileList if any(m in n for m in yearFilter)]\n",
    "print('There are {} files since year 2000.'.format(len(filteredFileList)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "NOAA collects data on an hourly basis for each buoy. This means that we could have 24\\*365 (**8,760**) records included in each file. We counted **11,167** txt.gz files post 2000 in this folder, which could translate into **98M** records!\n",
    "\n",
    "In the context of this Jupyter Notebook, we'll just handle the files in 2000. We'll use the full fileset once we run the ETL process on AWS EMR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n"
     ]
    }
   ],
   "source": [
    "# extract filelist in 2000 since this is the year for our test data.\n",
    "smallerList=[n for n in filteredFileList if '2000.txt.gz' in n]\n",
    "print(len(smallerList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Let's create our **Spark session** to investigate our larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "# use the command below to improve efficiency\n",
    "spark.conf.set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\n",
    "# only display Warning messages and above. Skips the INFO messages.\n",
    "spark.sparkContext.setLogLevel('WARN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Important Note:** Spark can load data directly from csv files. However, Spark doesn't currently support multiple delimiter characters and this is what we unfortunately face with our data as the fields are separated by multiple spaces. This applies to the buoy historical dataset only.\n",
    "To avoid this issue, we'll load the data into a pandas dataframe first, then load that data frame into a Spark dataframe.\n",
    "Also, we decided to pick the data at noon the day of each swim, so we can reduce our dataset before it gets processed by Spark. This accelerates our data ingestion process.\n",
    "\n",
    "Below is an example on how Spark loads a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YYYY: long (nullable = true)\n",
      " |-- MM: long (nullable = true)\n",
      " |-- DD: long (nullable = true)\n",
      " |-- hh: long (nullable = true)\n",
      " |-- WD: long (nullable = true)\n",
      " |-- WSPD: double (nullable = true)\n",
      " |-- GST: double (nullable = true)\n",
      " |-- WVHT: double (nullable = true)\n",
      " |-- DPD: double (nullable = true)\n",
      " |-- APD: double (nullable = true)\n",
      " |-- MWD: long (nullable = true)\n",
      " |-- BAR: double (nullable = true)\n",
      " |-- ATMP: double (nullable = true)\n",
      " |-- WTMP: double (nullable = true)\n",
      " |-- DEWP: double (nullable = true)\n",
      " |-- VIS: double (nullable = true)\n",
      " |-- TIDE: double (nullable = true)\n",
      " |-- buoyID: string (nullable = true)\n",
      "\n",
      "+----+---+---+---+---+----+---+----+----+----+---+------+----+----+----+----+----+------+\n",
      "|YYYY| MM| DD| hh| WD|WSPD|GST|WVHT| DPD| APD|MWD|   BAR|ATMP|WTMP|DEWP| VIS|TIDE|buoyID|\n",
      "+----+---+---+---+---+----+---+----+----+----+---+------+----+----+----+----+----+------+\n",
      "|2000|  1|  1|  1|141| 1.8|2.2|99.0|99.0|99.0|999|1020.3|20.4|20.7|14.6|99.0| NaN| 42036|\n",
      "|2000|  1|  1|  2|123| 3.5|4.0|99.0|99.0|99.0|999|1020.3|20.4|20.7|15.5|99.0| NaN| 42036|\n",
      "|2000|  1|  1|  3|133| 3.8|4.6|99.0|99.0|99.0|999|1020.5|20.4|20.7|16.3|99.0| NaN| 42036|\n",
      "|2000|  1|  1|  4|134| 4.2|4.9|99.0|99.0|99.0|999|1020.5|20.3|20.6|16.6|99.0| NaN| 42036|\n",
      "|2000|  1|  1|  5|145| 4.3|4.9|99.0|99.0|99.0|999|1020.3|20.4|20.6|17.4|99.0| NaN| 42036|\n",
      "+----+---+---+---+---+----+---+----+----+----+---+------+----+----+----+----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define buoy data schema to improve performance\n",
    "dfBuoy = spark.createDataFrame(df)\n",
    "\n",
    "dfBuoy.printSchema()\n",
    "dfBuoy.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//42a02h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//42a03h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//42otph2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//41001h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//41002h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//41004h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//41008h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//41009h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//41010h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//42001h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//42002h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//42003h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//42007h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//42019h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//42020h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//42035h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//42036h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//42039h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//42040h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//42041h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//42042h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//42054h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//44004h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//44005h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//44007h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//44008h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//44009h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//44011h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//44013h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//44014h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//44025h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//45001h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//45002h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//45003h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//45004h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//45005h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//45006h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//45007h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//45008h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46001h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46002h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46005h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46006h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46011h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46012h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46013h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46014h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46022h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46023h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46025h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46026h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46027h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46028h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46029h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46030h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46035h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46041h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46042h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46047h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46050h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46053h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46054h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46059h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46060h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46061h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46062h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46063h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//46066h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//48011h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//51001h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//51002h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//51003h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//51004h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//51028h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//aban6h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//alsn6h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//auga2h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//blia2h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//burl1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//buzm3h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//caro3h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//cdrf1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//chlv2h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//clkn7h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//csbf1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//dbln6h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//desw1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//disw3h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//dpia1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//drfa2h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//dryf1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//dsln7h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//ducn7h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//fbis1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//ffia2h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//fpsn7h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//fwyf1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//gdil1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//glln6h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//iosn3h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//ktnf1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//lkwf1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//lonf1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//mdrm1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//mism1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//mlrf1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//mrka2h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//nwpo3h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//pila2h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//pilm4h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//pota2h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//ptac1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//ptat2h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//ptgc1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//roam4h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//sanf1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//sauf1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//sbio1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//sgnw3h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//sisw1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//smkf1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//spgf1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//srst2h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//stdm4h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//supn6h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//thin6h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//tplm2h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//ttiw1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//venf1h2000.txt.gz\n",
      "Processing file: https://www.ndbc.noaa.gov/data/historical/stdmet//wpow1h2000.txt.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42039"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx,f in enumerate(smallerList):\n",
    "    url = f\n",
    "    ext = '.txt.gz'\n",
    "    print('Processing file:', url)\n",
    "    df = pd.read_csv(url, sep='\\s+')\n",
    "    # extract the buoyID from the file path then add it as a column to the Dataframe.\n",
    "    buoyID=url.rsplit('h', 1)[0].split('/')[-1]\n",
    "    df['buoyID'] = buoyID\n",
    "    df_filtered = df[df['hh'] == 12]  #only keeping 12 o'clock records.\n",
    "    dftemp = spark.createDataFrame(df_filtered)\n",
    "    if idx == 0 :\n",
    "        dfBuoy=dftemp\n",
    "    else:\n",
    "        dfBuoy=dfBuoy.union(dftemp)\n",
    "dfBuoy.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+---+---+----+----+----+----+----+---+------+----+----+-----+----+----+------+\n",
      "|YYYY| MM| DD| hh| WD|WSPD| GST|WVHT| DPD| APD|MWD|   BAR|ATMP|WTMP| DEWP| VIS|TIDE|buoyID|\n",
      "+----+---+---+---+---+----+----+----+----+----+---+------+----+----+-----+----+----+------+\n",
      "|2000| 12|  1| 12|121| 5.2| 6.9|1.21|5.88|4.76| 78|1018.9|23.5|24.6|999.0|99.0|99.0| 42a02|\n",
      "|2000| 12|  2| 12|356| 3.9| 5.2|0.95|7.69| 5.8|105|1021.0|23.2|24.5|999.0|99.0|99.0| 42a02|\n",
      "|2000| 12|  3| 12| 29|10.5|13.1|2.48|7.69|6.05|  5|1025.8|17.6|24.5|999.0|99.0|99.0| 42a02|\n",
      "|2000| 12|  4| 12| 43| 6.3| 7.2|1.38|6.67|5.27| 55|1025.2|19.1|24.5|999.0|99.0|99.0| 42a02|\n",
      "|2000| 12|  5| 12| 21| 8.8|11.1|1.36|5.88| 5.1| 51|1026.0|19.0|24.9|999.0|99.0|99.0| 42a02|\n",
      "+----+---+---+---+---+----+----+----+----+----+---+------+----+----+-----+----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfBuoy.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Swimslog data\n",
    "We artificially created a swims log data set to simulate the way swimmers would check in/out to a swim.\n",
    "The data is provided in a JSON file format for each month since 2000. As mentioned before, we have **2+ Million**  records. Let's see if the file can be loaded by Spark locally and display the first few records.\n",
    "In the context of this notebook, we'll explore data in the data/swimslog/2000 folder. In production (on AWS EMR), we'll process 20 years of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- eventID: string (nullable = true)\n",
      " |-- eventLocation: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- swimID: long (nullable = true)\n",
      " |-- swimStatus: string (nullable = true)\n",
      " |-- swimmer: string (nullable = true)\n",
      " |-- swimmerEmail: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSwimslog =spark.read.json('data/swimslog/2000/*.json', multiLine=True)\n",
    "dfSwimslog.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+------+----------+------------------+--------------------+\n",
      "|             eventID|       eventLocation|eventTime|swimID|swimStatus|           swimmer|        swimmerEmail|\n",
      "+--------------------+--------------------+---------+------+----------+------------------+--------------------+\n",
      "|68EB407E-9F70-B21...| -60.64933, 99.93537|973537720| 53711|        No|     Quon Mcclure |egestas.a.dui@atp...|\n",
      "|150E9B7E-56BE-1E3...|  -54.64544, 78.4572|975202729| 51218|        No|     Hyatt Mooney |nostra@luctussit....|\n",
      "|B290039C-60AB-55A...|-76.63623, 104.18138|974452207| 50427|       Yes| Penelope Delgado |Nullam@Donecnibh....|\n",
      "|DF54C0CD-1A47-235...| 35.92507, -39.74448|974657087| 50239|        No|     Chaney Reese |pede@SeddictumPro...|\n",
      "|6A2A4DD6-5EE8-8B8...|-13.06251, 166.54162|975074884| 52405|        No|    Charity Gould |Donec.egestas.Dui...|\n",
      "+--------------------+--------------------+---------+------+----------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSwimslog.show(5)\n",
    "dfSwimslog.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As seen above, the swimslog records are (in theory) uniquely identified. They include an eventTime (expressed in timestamp) and location (expressed in long/lat coordinates) when the swimmer checked in/out. The swimslog records also refer to a swimID. The swimmer full name and email address are included along with the status which indicates if the swimmer participated or not in this swim. Finally, in the case above, we only processed 120K records which is what we had for year 2000 only.\n",
    "\n",
    "#### Swims data set\n",
    "We also have access to all the swims that happened since 2000. This is another artificially generated file.\n",
    "As shown below, the data is structured using a JSON format. The BuoyID is a key aspect of the swim because it allows us to understand the swimming conditions provided by the NOAA data.\n",
    "This swims data set includes **1.3M** records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BuoyID: string (nullable = true)\n",
      " |-- Coordinates: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- swimID: long (nullable = true)\n",
      "\n",
      "+------+--------------------+----------+--------------+------+\n",
      "|BuoyID|         Coordinates|      Date|      Location|swimID|\n",
      "+------+--------------------+----------+--------------+------+\n",
      "| 44089|43.10171, -161.63003|08/25/2000|   Blairgowrie| 38026|\n",
      "| mxxa2|  46.54514, 96.71354|08/08/2000| Cercemaggiore| 36701|\n",
      "| gbcl1|  6.87131, 151.53133|08/16/2000|           Pau| 38616|\n",
      "| dpxc1|-30.22349, 161.03706|08/22/2000|     Wałbrzych| 39207|\n",
      "| 18cy3| 78.00928, 160.68506|08/19/2000|Neubrandenburg| 37999|\n",
      "+------+--------------------+----------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSwims =spark.read.json('data/swims/*/*.json', multiLine=True)\n",
    "dfSwims.printSchema()\n",
    "dfSwims.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Some swims are sometimes associated with two different buoys. \n",
    "# In theory, this shouldn't be the case , so we're dropping dups.\n",
    "dfSwims=dfSwims.dropDuplicates(['swimID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning & Formatting Steps\n",
    "\n",
    "- dfSwims was provided with a simple date format. To align better with other data sets, we *exploded* this date formats into its main components (year, month, day). See below.\n",
    "\n",
    "- Buoy data format changed over time. We need to accommodate both old and new data formats. We'll manage this problem through some transformations in our etl.py script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+----------+---------------+------+----+-----+---+\n",
      "|BuoyID|         Coordinates|      Date|       Location|swimID|Year|Month|Day|\n",
      "+------+--------------------+----------+---------------+------+----+-----+---+\n",
      "| 52212| -84.22602, 65.75303|01/26/2000|            Sim|    26|2000|   01| 26|\n",
      "| 14041| 79.0899, -117.79272|01/14/2000|      Rochester|    29|2000|   01| 14|\n",
      "| 32069| 24.44656, 128.04441|01/02/2000|   Ponta Grossa|   474|2000|   01| 02|\n",
      "| hplm2| 44.26266, -175.5417|01/19/2000|Tione di Trento|   964|2000|   01| 19|\n",
      "| ducn7|-66.48567, -147.2...|01/16/2000|       Zlatoust|  1677|2000|   01| 16|\n",
      "+------+--------------------+----------+---------------+------+----+-----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert date string to date yyyy/MM/dd\n",
    "dfSwims=dfSwims.withColumn(\"Year\", date_format(to_date(col(\"Date\"),\"MM/dd/yyyy\"), \"yyyy\"))\\\n",
    "                .withColumn(\"Month\", date_format(to_date(col(\"Date\"), \"MM/dd/yyyy\"), \"MM\"))\\\n",
    "                .withColumn(\"Day\", date_format(to_date(col(\"Date\"), \"MM/dd/yyyy\"), \"dd\"))\n",
    "dfSwims.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "We currently have disconnected databases and our goal is to create a **STAR** database schema where all relevant information is connected together. The STAR schema is an efficient architecture to record all swims and access data efficiently.\n",
    "\n",
    "We have a **FACT** table which includes all the swims and some **DIMENSION** tables which provide more information on those swims. Dimension tables are: *Swimmers*, *Locations*, *Conditions* and *Time* tables.\n",
    "\n",
    "A Database diagram can be found [here](./Documentation/DataModel.pdf).\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Here are the steps included in our data pipeline to build our STAR database. Each table will be written to the './data/output/' folder as part of this Jupyter Notebook. Tables can also be written to AWS S3 by using the etl.py script and setting the LOCATION parameter in the dl.cfg file to S3 (as opposed to LOCAL).\n",
    "\n",
    "Using the Swimslog, Swims, Buoy data available, we take on the following actions: \n",
    "\n",
    "1. Create and write Swimmers table\n",
    "2. Create and write Locations table\n",
    "3. Create and write Conditions table\n",
    "4. Create and write Time table\n",
    "5. Create and write Swims table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- swimmerIdx: long (nullable = false)\n",
      "\n",
      "+---------+--------+--------------------+----------+\n",
      "|firstName|lastName|               Email|swimmerIdx|\n",
      "+---------+--------+--------------------+----------+\n",
      "|   Cedric|Bradshaw|Suspendisse@Proin...|         0|\n",
      "|  Charity|  Willis|     et@primisin.org|         1|\n",
      "|     Clio| Russell|ante.lectus.conva...|         2|\n",
      "|   Denton|  Cotton|lobortis.Class@li...|         3|\n",
      "|   Sierra| Skinner|consectetuer.maur...|         4|\n",
      "+---------+--------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- locationIdx: long (nullable = false)\n",
      "\n",
      "+-----------------+---------+----------+-----------+\n",
      "|         Location|Longitude|  Latitude|locationIdx|\n",
      "+-----------------+---------+----------+-----------+\n",
      "| Altavilla Irpina|-71.70639| 161.18838|          0|\n",
      "|        Bangalore|-54.94474| -25.77683|          1|\n",
      "|           Bingen| 12.12119| -48.54922|          2|\n",
      "|        Cajamarca|-26.95382| 148.00585|          3|\n",
      "|Cappelle sul Tavo| 16.08573|   4.98617|          4|\n",
      "+-----------------+---------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- buoyID: string (nullable = true)\n",
      " |-- ATMP: double (nullable = true)\n",
      " |-- WTMP: double (nullable = true)\n",
      " |-- TIDE: double (nullable = true)\n",
      " |-- WVHT: double (nullable = true)\n",
      " |-- WD: long (nullable = true)\n",
      " |-- WSPD: double (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- conditionsIdx: long (nullable = false)\n",
      "\n",
      "+------+----+----+----+----+---+----+----+-----+---+-------------+\n",
      "|buoyID|ATMP|WTMP|TIDE|WVHT| WD|WSPD|Year|Month|Day|conditionsIdx|\n",
      "+------+----+----+----+----+---+----+----+-----+---+-------------+\n",
      "| 41004|26.4|27.1|99.0| 1.1| 57| 0.8|2000|   09| 20|            0|\n",
      "| 42003|24.6|27.1|99.0|0.62| 39| 5.7|2000|   10| 28|            1|\n",
      "| 42039|28.9|30.3| NaN|0.25|194| 1.2|2000|   07| 31|            2|\n",
      "| 42040|17.0|20.7| NaN|0.48| 37| 6.1|2000|   03| 05|            3|\n",
      "| 44011| 9.5| 8.2| NaN|0.66|303| 4.0|2000|   05| 16|            4|\n",
      "+------+----+----+----+----+---+----+----+-----+---+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- YYYY: string (nullable = true)\n",
      " |-- MM: string (nullable = true)\n",
      " |-- DD: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      "\n",
      "+----+---+---+----------+\n",
      "|YYYY| MM| DD|      Date|\n",
      "+----+---+---+----------+\n",
      "|2000| 06| 16|06/16/2000|\n",
      "|2000| 08| 24|08/24/2000|\n",
      "|2000| 03| 24|03/24/2000|\n",
      "|2000| 10| 20|10/20/2000|\n",
      "|2000| 01| 05|01/05/2000|\n",
      "+----+---+---+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- swimmerIdx: long (nullable = false)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- locationIdx: long (nullable = false)\n",
      " |-- WTMP: double (nullable = true)\n",
      " |-- conditionsIdx: long (nullable = true)\n",
      " |-- YYYY: string (nullable = true)\n",
      " |-- MM: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- swimStatus: string (nullable = true)\n",
      " |-- SwimslogIdx: long (nullable = false)\n",
      "\n",
      "+---------+---------+-------------+--------------+-------------+-----+-------------+----+---+----------+----------+-----------+\n",
      "|firstName| lastName|   swimmerIdx|      Location|  locationIdx| WTMP|conditionsIdx|YYYY| MM|      Date|swimStatus|SwimslogIdx|\n",
      "+---------+---------+-------------+--------------+-------------+-----+-------------+----+---+----------+----------+-----------+\n",
      "|    Kerry|Jefferson|  51539607554|    Vanderhoof|1168231104524|999.0| 652835029004|2000| 11|11/29/2000|        No|          0|\n",
      "|    Kerry|Jefferson|  51539607554|    Vanderhoof|1168231104524|999.0| 652835029011|2000| 11|11/29/2000|        No|          1|\n",
      "|  Vincent|  Walters|1254130450439|      Nocciano| 283467841566| null|         null|2000| 11|11/03/2000|       Yes|          2|\n",
      "| Kathleen|  Leblanc|   8589934593|Coleville Lake|1030792151045| null|         null|2000| 09|09/08/2000|       Yes|          3|\n",
      "|     John|  Delgado|1580547964932|        Virton| 747324309510| 10.6| 438086664209|2000| 11|11/21/2000|       Yes|          4|\n",
      "+---------+---------+-------------+--------------+-------------+-----+-------------+----+---+----------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create swimmers_table using dfSwimslog data\n",
    "df2=dfSwimslog.dropDuplicates(['swimmer'])\n",
    "swimmers_table=df2.select(split(col(\"swimmer\"),'\\s+').getItem(1).alias(\"firstName\"), \\\n",
    "                    split(col(\"swimmer\"),'\\s+').getItem(2).alias(\"lastName\"), col(\"swimmerEmail\").alias(\"Email\") )\\\n",
    "                  .withColumn(\"swimmerIdx\", monotonically_increasing_id())\n",
    "\n",
    "swimmers_table.printSchema()\n",
    "swimmers_table.show(5)\n",
    "\n",
    "# write swimmers_table\n",
    "swimmers_table.write.mode('overwrite').parquet('./data/swimmers/swimmers_table.parquet')\n",
    "\n",
    "# create location table using locations data from swims dataframe \n",
    "locations_table=dfSwims.select(col(\"Location\"), split(col(\"Coordinates\"), ',').getItem(0).alias(\"Longitude\"), \\\n",
    "                                split(col(\"Coordinates\"), ',').getItem(1).alias(\"Latitude\"))\\\n",
    "                       .dropDuplicates(['Location']).withColumn(\"locationIdx\", monotonically_increasing_id())\n",
    "locations_table.printSchema()\n",
    "locations_table.show(5)\n",
    "\n",
    "# write locations_table\n",
    "locations_table.write.mode('overwrite').parquet('./data/locations/locations_table.parquet')\n",
    "\n",
    "# create conditions table using buoys and swims data\n",
    "conditions_table=dfSwims.join(dfBuoy, ((dfBuoy['buoyID'] == dfSwims['BuoyID']) & \\\n",
    "                                        (dfBuoy['YYYY'] == dfSwims['Year']) & \\\n",
    "                                       (dfBuoy['MM'] == dfSwims[\"Month\"]) & \\\n",
    "                                        (dfBuoy['DD'] == dfSwims['Day']))) \\\n",
    "                         .select(dfBuoy.buoyID,\n",
    "                                    dfBuoy.ATMP, \n",
    "                                    dfBuoy.WTMP,\n",
    "                                    dfBuoy.TIDE,\n",
    "                                    dfBuoy.WVHT,\n",
    "                                    dfBuoy.WD,\n",
    "                                    dfBuoy.WSPD,\n",
    "                                    dfSwims.Year,\n",
    "                                    dfSwims.Month,\n",
    "                                    dfSwims.Day)\\\n",
    "                         .withColumn(\"conditionsIdx\", monotonically_increasing_id())\n",
    "\n",
    "conditions_table.printSchema()\n",
    "conditions_table.show(5)\n",
    "\n",
    "# write conditions_table\n",
    "conditions_table.write.partitionBy(\"Year\", \"Month\", \"BuoyID\").mode('overwrite').parquet('./data/conditions/conditions_table.parquet')\n",
    "\n",
    "# extract columns to create time table\n",
    "time_table = dfSwims.select(col('Year').alias('YYYY'), col('Month').alias('MM'), \\\n",
    "                            col('Day').alias('DD'),col('Date')).dropDuplicates()\n",
    "time_table.printSchema()\n",
    "time_table.show(5)\n",
    "\n",
    "# write time table to parquet files partitioned by year and month\n",
    "time_table.write.partitionBy(\"YYYY\", \"MM\").mode('overwrite').parquet('./data/time/time_table.parquet')\n",
    "\n",
    "# create and write swims_table\n",
    "swims_table = dfSwimslog.join(dfSwims, (dfSwimslog.swimID == dfSwims.swimID))\\\n",
    "                        .join(swimmers_table, (dfSwimslog.swimmerEmail == swimmers_table.Email)) \\\n",
    "                        .join(time_table, (dfSwims.Date == time_table.Date)) \\\n",
    "                        .join(conditions_table, ((dfSwims.BuoyID == conditions_table.buoyID) & \\\n",
    "                                                (conditions_table.Year == time_table.YYYY) & \\\n",
    "                                                (conditions_table.Month == time_table.MM) & \\\n",
    "                                                (conditions_table.Day == time_table.DD)), 'left_outer') \\\n",
    "                        .join(locations_table, (dfSwims.Location == locations_table.Location)) \\\n",
    "                        .select(swimmers_table.firstName,swimmers_table.lastName, \\\n",
    "                                swimmers_table.swimmerIdx,locations_table.Location,locations_table.locationIdx,\\\n",
    "                                conditions_table.WTMP, conditions_table.conditionsIdx, time_table.YYYY, time_table.MM, time_table.Date, dfSwimslog.swimStatus)\\\n",
    "                        .dropDuplicates()\\\n",
    "                        .withColumn(\"SwimslogIdx\", monotonically_increasing_id())\n",
    "\n",
    "swims_table.printSchema()\n",
    "swims_table.show(5)\n",
    "\n",
    "# write swims_table\n",
    "swims_table.write.partitionBy('YYYY','MM','locationIdx').mode('overwrite').parquet('./data/swims/swims_table.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The table above shows some valid results. We can see that the first two records are almost identical. This is because NOAA increased the frequency on when they pick up the measurements. Initially, it was hourly and now multiple times per hour. We can only observed some null conditions. This means that the buoy didn't provide any data on that day and time. Buoys are sometimes non-operational or in maintenance for a few weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Once the data runs through the pipeline, we can perform some quality checks. We decided to check the following conditions:\n",
    "- verify the number of rows in each table\n",
    "- verify that there was no duplicated keys\n",
    "\n",
    "Here is the code to check the validity of the data. We'll change those parameters in production as the number of records will increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing number of rows on: swimmers table\n",
      "Testing number of rows on: buoys\n",
      "Testing number of rows on: conditions table\n",
      "Testing number of rows on: swims table\n",
      "Testing number of rows on: time table\n",
      "Testing number of rows on: locations table\n",
      "Testing key uniqueness on: swimmers table\n",
      "Testing key uniqueness on: swims table\n",
      "Testing key uniqueness on: conditions table\n",
      "Testing key uniqueness on: time table\n",
      "Testing key uniqueness on: locations table\n",
      "All tests completed!\n"
     ]
    }
   ],
   "source": [
    "def test_min_numb_rows(area, df, target):\n",
    "    print(\"Testing number of rows on:\", area)\n",
    "    assert df.count() >= target, \"Number of rows in {} is lower than expected\".format(area)\n",
    "\n",
    "def test_key_uniqueness(area, df, primaryKey):\n",
    "    print(\"Testing key uniqueness on:\", area)\n",
    "    assert df.select(primaryKey).distinct().count() == df.count(), \"Multiple rows use the same primary key\".format(area)\n",
    "\n",
    "# Testing min number of rows for each key table\n",
    "test_min_numb_rows('swimmers table', swimmers_table, 1001)\n",
    "test_min_numb_rows('buoys', dfBuoy, 40000)\n",
    "test_min_numb_rows('conditions table', conditions_table, 100)\n",
    "test_min_numb_rows('swims table', swims_table, 1)\n",
    "test_min_numb_rows('time table', time_table, 30)\n",
    "test_min_numb_rows('locations table', locations_table, 100)\n",
    "\n",
    "# Testing primary key uniqueness\n",
    "test_key_uniqueness ('swimmers table', swimmers_table, 'swimmerIdx')\n",
    "test_key_uniqueness ('swims table', swims_table, 'swimslogIdx')\n",
    "test_key_uniqueness ('conditions table', conditions_table, 'conditionsIdx')\n",
    "test_key_uniqueness ('time table', time_table, 'Date')\n",
    "test_key_uniqueness ('locations table', locations_table, 'locationIdx')\n",
    "\n",
    "print(\"All tests completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "A data dictionary is available [here](./data/Documentation/OpenWaterSwimDataDictionary.pdf). It provides a description for each data element of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 5: Running the Pipeline on AWS EMR\n",
    "The ultimate goal is to run the pipeline on AWS EMR. This requires several steps:\n",
    "- Set up AWS EMR instance\n",
    "- Copy **ETL.py** and **DL.cfg** scripts to AWS EMR\n",
    "- Launch **ETL.py** script on AWS EMR\n",
    "\n",
    "This also assumes that all our data is available on AWS S3.\n",
    "We made our [S3 bucket](s3://openwaterswimming/input/) public in case anyone would like to use/test the data. Will turn it back to private once this project is approved.\n",
    "\n",
    "#### Step 5.1: Set up AWS EMR instance\n",
    "Using the AWS Console, we launch an AWS EMR instance. We opted for the 5.28 configuration with SPARK.\n",
    "We decided to pick a regular configuration (m5.xlarge) using 4 nodes (1 master and 3 slaves). We don't need super powerful machines because we don't operate too many operations. However, it's good to have multiple machines since we process a lot of data.\n",
    "\n",
    "Once we launch AWS EMR, we can access it via the terminal using the following command:\n",
    "ssh -i openwaterswim.pem hadoop@IPaddress where IPaddress is the AWS EMR IP address.\n",
    "\n",
    "We need to install a few Python packages that are used by our scripts:\n",
    "- configparser\n",
    "- requests\n",
    "- pandas\n",
    "\n",
    "We install those packages by using the following command:\n",
    "* sudo pip install *eachpackage*\n",
    "\n",
    "#### Step 5.2: Copy ETL.py and DL.cfg scripts\n",
    "To copy the scripts to AWS EMR, we use the scp command as follows:\n",
    "\n",
    "* scp -i openwater.pem etl.py hadoop@IPaddress:/home/hadoop/\n",
    "* scp -i openwater.pem dl.cfg hadoop@IPaddress:/home/hadoop/\n",
    "\n",
    "where IPaddress is the AWS EMR IPaddress.\n",
    "\n",
    "#### Step 5.3: Launch ETL.py script via spark-submit\n",
    "To launch the ETL.py script on AWS EMR instance, we first need to run the following command:\n",
    "* **export PYTHONIOENCODING=utf-8** (this avoids an encoding issue with our data)\n",
    "* **spark-submit etl.py**\n",
    "\n",
    "#### Step 5.4: Verify the results\n",
    "The **FACT** and **DIMENSION** tables are written on AWS S3 as expected. The ETL process runs pretty quickly ~15M records for our 3 key datasets: buoys, swims and swimlogs.\n",
    "The process also performs quality checks defined in section 4.2 of this document.\n",
    "\n",
    "#### Important Note\n",
    "- The ETL process can also be run locally (with Spark). However, due to the large dataset, it is recommended to only run it with the local data (year 2000). We actually tested to connect with S3 from the provided environment, but it seems that the environment was missing some packages. It is able to connect to S3 repository using our EMR configuration above.\n",
    "- When running locally, we definitely suggest to set the buoy range to 2000. This can be set in the dl.cfg file in the TIMEFRAME section with the MIN_RANGE and MAX_RANGE parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 6: Other Considerations\n",
    "In this section, we'll cover a few more topics such as:\n",
    "- Technology choices\n",
    "- Data refresh frequency issue\n",
    "- Scalability issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 6.1 Technology Choices\n",
    "For this project, we decided to use Pandas, Spark, AWS EMR and AWS S3 for the following reasons:\n",
    "1. **Pandas** - We couldn't directly process the buoy dataset with Spark because it uses multiple spaces as a separator. This is a known limitation which will be addressed in future versions of Spark. So, we decided to first process the data with Pandas and then load those data frames into Spark. This is a temporary turn-around as it slows down the processing of all buoy data.\n",
    "\n",
    "2. **Spark** - We needed a tool capable of processing multi-million records datasets. Spark is built for that purpose.\n",
    "3. **AWS EMR** - While we can run Spark locally (on a single machine), it's better to run it on a cluster. We decided to set up an AWS EMR cluster and immediately saw the benefit from a performance perspective.\n",
    "4. **AWS S3** - We work with big data sets. Also, the dataset keeps increasing every hour, so we need an elastic storage solution. Storing on AWS S3 also allows us to access the data from multiple tool. For example, we can write to S3 and access the data from a tool like AWS redshift.\n",
    "\n",
    "#### 6.2 Data Refresh Frequency\n",
    "Open Water Swimming activities happen frequently. Also, buoys data get updated on an hourly basis. If we want to capture insights about open water swimming activities, we can update the data on a monthly basis. If we'd like to use the latest data, then we suggest to update on an hourly basis. That said, it would probably be better to leverage a technology like Kafka which offers a subscription mechanism. That way, data would be available real-time.\n",
    "\n",
    "#### 6.3 Scalability issues\n",
    "How would we address the following scalability scenari:\n",
    "1. Data was increased by 100%\n",
    "We don't anticipate too many problems with such volume increase. We use elastic technologies that allows us to scale rapidly. For example, AWS S3 would scale immediately. AWS EMR didn't show any performance issue with our current load. If it started to slow down, we would recommend to add a node to our cluster.\n",
    "\n",
    "2. Data populates a dashboard which needs to be refreshed daily by 7 AM\n",
    "In such situation, we recommend to create a DAG in Apache Airflow. The process can be scheduled and verified on a daily basis. If a problem occurs, we would immediately be notified.\n",
    "\n",
    "3. Database needs to be accessed by 100+ people\n",
    "If the database needs to be accessed by many people and possibly geographically dispersed around the world, we recommend to use a Data Warehouse technology like AWS Redshift or Snowflake. We also recommend to use a non-relationtial database like Cassandra, so the data can be fragmented geographically for example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 7: Final Thoughts\n",
    "I enjoyed this project very much despite the numerous challenges. This project was challenging because I needed to research information on publicly available datasets. I also took the path to generate my own data which came with its own challenges. For example, initial data sets were not truly comformed to JSON formats; some generated data included unexpected characters, etc...\n",
    "NOAA data also presented some challenges with the data formats and the fact that they do change over time. When we look at 20+ years old data, things have chanegd. A data pipeline needs to address those aspects.\n",
    "Running the ETL process on the entire dataset takes multiple hours. The buoy data is currently loaded sequentially (due to the Pandas dependency). This is something that can be improved in the future.\n",
    "\n",
    "All in all, the **Udacity Nano-degree in Data Engineering** provided a comprehensive aspect on many data engineering activities, challenges, technologies and best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
